{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455b6ae0-55de-4d2b-b6ee-0f4ac3434896",
   "metadata": {},
   "source": [
    "# Interleaving Simulation and Steering\n",
    "We use a batch strategy [in our previous example](./0_molecular-design-with-parsl.ipynb) that, while simple to implement, leads to under-utilization.\n",
    "The core problem of a batch strategy is that only one type of task - simulation, training, or inference - at a single time.\n",
    "The serial nature results in several points during the workflow where either there are not enough tasks (e.g., one model to train)\n",
    "or tail-down loses while we wait for the last tasks from a batch to complete before starting the next type.\n",
    "This example shows how to increase parallelism by using [Colmena](https://colmena.readthedocs.io/en/latest/) to run multiple kinds of tasks concurrently.\n",
    "\n",
    "> **NOTE**: Make sure to start Redis before running this notebook if it is not already running on your system.\n",
    "```bash\n",
    "redis-server redis.conf &> redis.log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7e9dd4-4206-42dc-92d8-1574e14e6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import widgets\n",
    "from colmena.models import Result\n",
    "from colmena.task_server.parsl import ParslTaskServer\n",
    "from colmena.redis.queue import make_queue_pairs\n",
    "from colmena.thinker.resources import ResourceCounter\n",
    "from colmena.thinker import BaseThinker, event_responder, task_submitter, result_processor\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.config import Config\n",
    "from random import shuffle\n",
    "from time import perf_counter\n",
    "from threading import Lock, Event\n",
    "from typing import List\n",
    "from chemfunctions import compute_vertical, train_model, run_model\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1ba1c-b4a0-4d6f-ac30-4761c34c8868",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d8cbca-69ab-47ee-b9c0-da374f6e0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = min(4, os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7e9df-2992-45a7-b077-4ddca5163362",
   "metadata": {},
   "source": [
    "Log the Colmena output to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22098bae-0d02-4b43-a4ef-2068b525509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "handlers = [logging.FileHandler('colmena.log', mode='w')]\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO, handlers=handlers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8408f6-247d-41c9-be5a-957c04699547",
   "metadata": {},
   "source": [
    "## Load in the Data\n",
    "We're going to use the same problem as the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf35efa-6f01-45fb-a788-1a839ea1441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = pd.read_csv('data/QM9-search.tsv', delim_whitespace=True)  # Our search space of molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3a9145-4cdb-4178-bcf9-1254486e4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count: int = 8  # Number of calculations to run at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cfaba0e-a191-492d-b4e0-976210f30cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_count: int = 64   # Number of molecules to evaluate in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4b51cb-8ee4-47e2-85fd-b12ca3054696",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 4  # Number of molecules to evaluate in each batch of simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165328c-de76-4d96-8053-1ba3297989c3",
   "metadata": {},
   "source": [
    "## Building a Colmena Application\n",
    "Colmena applications have three parts: a _Task Server_ that manages execution of computations at the direction of a _Thinker_ through a _Task Queue_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05680f7-6c57-4dce-8bf6-d3bb3e08981f",
   "metadata": {},
   "source": [
    "### Creating Task Queue\n",
    "A task queue is responsible for conveying requests to perform a computation to a Task Server, and then supplying results back to the Thinker.\n",
    "Creating a task queue requires defining connection information to Redis and the names of separate topics used to separate different kinds of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab75772a-91c9-4ef3-8e30-a13f43da9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_queues, server_queues = make_queue_pairs(hostname='localhost', topics=['simulate', 'train', 'infer'], serialization_method='pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ee709-465f-40c5-9c27-9a5a25119660",
   "metadata": {},
   "source": [
    "### Defining Task Server\n",
    "The Task Server requires a task queue to communicate through, a list of methods, and a set of computational resources to run them on. (See [Colmena Docs](https://colmena.readthedocs.io/en/latest/how-to.html#configuring-a-task-server))\n",
    "\n",
    "The computation resources are defined using Parsl's definitions. We'll use the same one as the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693927c6-1a00-40e5-a291-729a3d7f805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    executors=[HighThroughputExecutor(\n",
    "        max_workers=n_workers, # Allows a maximum of two workers\n",
    "        cpu_affinity='block' # Prevents workers from using the same cores\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ada804-444c-4461-a064-816e47558bff",
   "metadata": {},
   "source": [
    "We supply a list of Python functions to define the methods and also give the constructor a link to the queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1688d4-1bcc-4fd5-902e-803afdf4caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_server = ParslTaskServer(\n",
    "    methods=[compute_vertical, train_model, run_model],\n",
    "    queues=server_queues,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803bca2-8aa9-4172-8314-59328cc4b767",
   "metadata": {},
   "source": [
    "The task server runs in the background. \n",
    "> *NOTE*: You must kill it before exiting the notebook by sending a kill signal. We do that in the last cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82fde165-0122-4d47-8ec7-23a92f2983c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3a74c-5c5a-4412-b36a-e7b026bdddf7",
   "metadata": {},
   "source": [
    "The server will run tasks on request from a queue and send them back on a different Redis queue.\n",
    "The client queue object provides a `send_inputs` and `get_result` method to perform these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a7293bc-f33f-41e1-bc51-b66333bebda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.61 ms, sys: 0 ns, total: 3.61 ms\n",
      "Wall time: 3.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "client_queues.send_inputs('C', method='compute_vertical')\n",
    "result = client_queues.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9220cd0f-0c78-4841-8ea0-a866d7425a49",
   "metadata": {},
   "source": [
    "Both accept a \"topic\" option that allows for multiplexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca72bba-62b1-44b9-b097-e3594295da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_queues.send_inputs('C', method='compute_vertical', topic='simulate')\n",
    "\n",
    "# Show that we do not pull results on other topics\n",
    "result = client_queues.get_result(topic='infer', timeout=15)\n",
    "assert result is None  # None means a timeout occurred\n",
    "\n",
    "# Pull from the correct queue\n",
    "result = client_queues.get_result(topic='simulate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b940d-d80f-46ac-9d95-606b857c0c2c",
   "metadata": {},
   "source": [
    "## Building a Thinker\n",
    "The Thinker part of a Colmena application coordinates what tasks are run by the Task Server.\n",
    "\n",
    "Thinker applications are built using a collection of threads (\"agents\") that cooperate to perform some task. \n",
    "For example, you can have an agent that records a simulation being completed and launches a second agent that manages retraining the models.\n",
    "\n",
    "Below, we walk through how to build a thinker application though progressively more complex examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3171fc7-53ee-4cc4-8db6-047a967d20aa",
   "metadata": {},
   "source": [
    "### Example 1: Simulating molecules in a predefined list\n",
    "A steering policy in Colmena is defined through a [Thinker](https://colmena.readthedocs.io/en/latest/how-to.html#creating-a-thinker-application) class. \n",
    "\n",
    "The Thinker class has methods which can run as parallel threads and share information with each other via class attributes, \n",
    "which always includes a [\"resource counter\"](https://colmena.readthedocs.io/en/latest/thinker.html#resource-counter) used to signal when resources are free.\n",
    "We use the `ResourceCounter` in this example to only submit as many tasks as we have workers so that we submit tasks based on the most up-to-date guidance from machine learning.\n",
    "\n",
    "We denote which functions in a Thinker are agents using decorators for the methods (e.g., `@agent`).\n",
    "Each of the functions marked with these decorators will started as threads when `.run` or `.start` is called.\n",
    "\n",
    "Colmena provides [many kinds of decorators for common types of agents](https://colmena.readthedocs.io/en/latest/thinker.html#special-purpose-agents). \n",
    "In this demo, we use three of them:\n",
    "\n",
    "- `task_submitter` runs when resources are available.\n",
    "- `result_processor` runs when a certain topic of task completes\n",
    "- `event_responder` runs when an [`Event`](https://docs.python.org/3/library/threading.html#event-objects) is set\n",
    "\n",
    "A simple example for a Thinker is one that submits a new calculation from a list when another completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70e58d4f-0095-4365-863b-1831aca433ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomThinker(BaseThinker):\n",
    "    \"\"\"A thinker which evaluates molecules in a random order.\"\"\"\n",
    "    \n",
    "    def __init__(self, queues, n_to_evaluate: int, n_parallel: int, \n",
    "                 molecule_list: List[str]):\n",
    "        \"\"\"Initialize the thinker\n",
    "        \n",
    "        Args:\n",
    "            queues: Client side of queues\n",
    "            n_to_evaluate: Number of molecules to evaluate\n",
    "            n_parallel: Number of computations to run in parallel\n",
    "            molecule_list: List of SMILES strings\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            queues, \n",
    "            # Establishes pools of resources for each kind of task\n",
    "            #  We'll only use the \"simulation\" pool\n",
    "            ResourceCounter(n_parallel, ['simulate', 'train', 'infer'])\n",
    "        )\n",
    "        \n",
    "        # Store the user settings\n",
    "        self.molecule_list = set(molecule_list)\n",
    "        self.n_to_evaluate = n_to_evaluate\n",
    "        \n",
    "        # Create a database of evaluated molecules\n",
    "        self.database = dict()\n",
    "        \n",
    "        # Create a record of completed calculations\n",
    "        self.simulation_results = []\n",
    "        \n",
    "        # Create a priority list of molecules, starting with them ordered randomly\n",
    "        self.priority_list = list(self.molecule_list)\n",
    "        shuffle(self.priority_list)\n",
    "        self.priority_list_lock = Lock()  # Ensures two agents cannot use it \n",
    "        \n",
    "        # Create a tracker for how many sent and how many complete\n",
    "        self.rec_progbar = tqdm(total=n_to_evaluate, desc='started')\n",
    "        self.sent_progbar = tqdm(total=n_to_evaluate, desc='successful')\n",
    "        \n",
    "        # Assign all of the resources over to simulation\n",
    "        self.rec.reallocate(None, 'simulate', n_parallel)\n",
    "\n",
    "    @task_submitter(task_type='simulate', n_slots=1)\n",
    "    def submit_calc(self):\n",
    "        \"\"\"Submit a calculation when resources are available\"\"\"\n",
    "        \n",
    "        with self.priority_list_lock:\n",
    "            next_mol = self.priority_list.pop()  # Get the next best molecule\n",
    "        \n",
    "        # Send it to the task server to run\n",
    "        self.queues.send_inputs(next_mol, method='compute_vertical')\n",
    "        self.rec_progbar.update(1)\n",
    "        \n",
    "    @result_processor\n",
    "    def receive_calc(self, result: Result):\n",
    "        \"\"\"Store the output of a run if it is successful\"\"\"\n",
    "        \n",
    "        # Mark that the resources are now free\n",
    "        self.rec.release('simulate', 1)\n",
    "        \n",
    "        # Store the result if successful\n",
    "        if result.success:\n",
    "            # Store the result in a database\n",
    "            self.database[result.args[0]] = result.value\n",
    "            \n",
    "            # Mark that we've received a result\n",
    "            self.sent_progbar.update(1)\n",
    "            \n",
    "            # If we've got all of the simulations complete, stop\n",
    "            if len(self.database) >= self.n_to_evaluate:\n",
    "                self.done.set()\n",
    "            \n",
    "        # Store the result object for later processing\n",
    "        self.simulation_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660f740-da57-44f0-a8d8-b6a084517323",
   "metadata": {},
   "source": [
    "We instantiate a copy of this thinker with the settings we want and then call `run` to start it working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19ee7272-546b-467c-a028-99041a9930d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99bc331871b4951945fba6570292ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "started:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da37acc46d3475ca6b5f334b713d964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "successful:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_thinker = RandomThinker(client_queues, search_count, n_workers, search_space['smiles'].values)\n",
    "random_thinker.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ef47c-ee54-4b0f-a7c7-0b64ac92843e",
   "metadata": {},
   "source": [
    "Watch how the thinker only starts new calculations after another one finishes. \n",
    "The ability to throttle will be important when we don't know which calculations to submit next until others have finished.\n",
    "\n",
    "> The thinker will receive more than the requested number of calculations, as we stop submitting only after enough have completed and wait until all submitted tasks complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203ac83-42b8-4e07-a042-5efcebd97d80",
   "metadata": {},
   "source": [
    "## Example 2: Batch optimization with slight overlap between simulation and ML\n",
    "We implement the same batch optimization as the Parsl example but with a slight addition: we start the ML tasks *while the simulation are still running*.\n",
    "\n",
    "The overlap between the simulation and ML tasks requires some conceptual changes in how we've approached workflows:\n",
    "1. *Concurrency in workflow planning*. Our tasks which manage simulation and ML act in parallel, which we manage with separate Colmena agents.\n",
    "1. *Multiplexed communication with task server*. The result queue now contains different types of task, which we separate with \"topic\" flags.\n",
    "1. *Workflow events change behavior*. Our machine learning tasks are launched only after enough data are acquired, which we trigger using the [Python threading library](https://docs.python.org/3/library/threading.html).\n",
    "    - Specifically, we use Events and use them to start agents using the [`event_responder` agent](https://colmena.readthedocs.io/en/latest/thinker.html#event-responder-agents)\n",
    "\n",
    "The example class, `BatchedThinker`, in [`thinkers.py`](./thinkers.py)  shows one way realizing the desired policy.\n",
    "\n",
    "> *WARNING*: We acknolwedge programming an object and debuging multi-threaded code is best done in an IDE. We present the completed result in a notebook for parismony, but recommend using a better programming environment for creating complex software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dbc33df-9732-42b1-a70a-5e3081a28608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkers import BatchedThinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f3325b-198b-4b74-a2ee-3fac13085ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00605724d724a588fb7fbf786a40322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325ab9f0b8ef46dbad39651fed94b2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "started:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d07f543dc6d4b22870c892f5df7cdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "successful:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training. Database size: 8...Training complete...Inference done. Elapsed time: 3.71s\n",
      "Starting training. Database size: 12...Training complete...Inference done. Elapsed time: 3.23s\n",
      "Starting training. Database size: 16...Training complete...Inference done. Elapsed time: 2.80s\n",
      "Starting training. Database size: 20...Training complete...Inference done. Elapsed time: 2.45s\n",
      "Starting training. Database size: 24...Training complete...Inference done. Elapsed time: 3.74s\n",
      "Starting training. Database size: 28...Training complete...Inference done. Elapsed time: 3.34s\n",
      "Starting training. Database size: 32...Training complete...Inference done. Elapsed time: 3.17s\n",
      "Starting training. Database size: 36...Training complete...Inference done. Elapsed time: 3.17s\n",
      "Starting training. Database size: 40...Training complete...Inference done. Elapsed time: 3.88s\n",
      "Starting training. Database size: 44...Training complete...Inference done. Elapsed time: 3.32s\n",
      "Starting training. Database size: 48...Training complete...Inference done. Elapsed time: 3.35s\n",
      "Starting training. Database size: 52...Training complete...Inference done. Elapsed time: 3.92s\n",
      "Starting training. Database size: 56...Training complete...Inference done. Elapsed time: 4.44s\n",
      "Starting training. Database size: 60...Training complete...Inference done. Elapsed time: 3.32s\n",
      "Starting training. Database size: 64...Training complete...Inference done. Elapsed time: 3.86s\n"
     ]
    }
   ],
   "source": [
    "output = widgets.Output()\n",
    "display(output)\n",
    "batched_thinker = BatchedThinker(\n",
    "    queues=client_queues,\n",
    "    n_to_evaluate=search_count,\n",
    "    n_parallel=n_workers,\n",
    "    initial_count=initial_count,\n",
    "    batch_size=batch_size,\n",
    "    molecule_list=search_space['smiles'].values,\n",
    "    dashboard=output\n",
    ")\n",
    "batched_thinker.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfca41-8ab2-44f8-a937-f0e89c326b6f",
   "metadata": {},
   "source": [
    "You can observe a few things about the policy from the notebook:\n",
    "1. Task submission is paused while training is on-going, and happens rapidly after the inference is complete. \n",
    "1. The thinker continues to record successful simulations while training is still underway.\n",
    "\n",
    "There is more detailed runtime information in the [`colmena.log`](./colmena.log). You can see the status messages as different agents start up and coordinate with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c44bb-24ca-4b2a-819b-f0cbfb53979a",
   "metadata": {},
   "source": [
    "## Wrap up Parsl\n",
    "Once complete, we send a \"kill\" signal to shutdown the task server. The task server will clean up any computational resources being used, then exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b3f14f9-8788-49e4-8e30-e8f442d4ce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process exited with 0 code\n"
     ]
    }
   ],
   "source": [
    "client_queues.send_kill_signal()\n",
    "task_server.join()\n",
    "print(f'Process exited with {task_server.exitcode} code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ae537-6a34-43d4-b211-39cd9c048d7e",
   "metadata": {},
   "source": [
    "## Take-home Points\n",
    "This example scratches the surface of the kind of steering policies you can write with Colmena.\n",
    "Colmena is a new code so examples of them are still growing, but include:\n",
    "\n",
    "1. Automatically re-allocating nodes between different tasks to maximize the effect of machine learning. ([paper](https://arxiv.org/pdf/2110.02827.pdf), [code](https://github.com/exalearn/electrolyte-design/tree/master/colmena/ip-single-fidelity), [data](https://doi.org/10.18126/bnfu-uk7f))\n",
    "1. Coordinating tasks between multiple compute sites. (*Paper under review*, [code](https://github.com/exalearn/multi-site-campaigns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebfdb3-4cec-42d1-983f-82fd04c721af",
   "metadata": {},
   "source": [
    "## Save Results for Later\n",
    "Lets save the run-traces for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f3f05ce-c1a8-47d4-b903-98353ce1d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('run-data/random-results.json', 'w') as fp:\n",
    "    for result in random_thinker.simulation_results:\n",
    "        print(result.json(), file=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8271d8d-7be6-4da2-a1f0-b0d48c82181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('run-data/batched-results.json', 'w') as fp:\n",
    "    for result in batched_thinker.simulation_results:\n",
    "        print(result.json(), file=fp)\n",
    "    for result in batched_thinker.learning_results:\n",
    "        # Write the learning results w/o the inputs and outputs\n",
    "        #  because they are not JSON-serializable\n",
    "        print(result.json(exclude={'inputs', 'value'}), file=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8903f3c-13b9-4632-9b9e-da6935d1265f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
